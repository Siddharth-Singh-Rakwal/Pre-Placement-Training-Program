{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063f8a0c-e921-48d9-9b08-e486b88f125d",
   "metadata": {},
   "source": [
    "# Assignment-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0a9eb-ee60-41bc-b966-62bde39423f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "696a8f6c-b1be-46ac-a0f5-b6b4c50c049c",
   "metadata": {},
   "source": [
    "# Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14302e19-4d56-4891-b81b-e43df6085f49",
   "metadata": {},
   "source": [
    "1. Design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms.\n",
    "2. Implement a real-time data ingestion pipeline for processing sensor data from IoT devices.\n",
    "3. Develop a data ingestion pipeline that handles data from different file formats (CSV, JSON, etc.) and performs data validation and cleansing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c0bb1-75a5-40e6-b735-8da71df8b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# Example code for data ingestion pipeline\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Database ingestion\n",
    "def ingest_from_database():\n",
    "    # Code for connecting to the database and extracting data\n",
    "    # ...\n",
    "\n",
    "    # Return the extracted data\n",
    "    return extracted_data\n",
    "\n",
    "# API ingestion\n",
    "def ingest_from_api():\n",
    "    # Code for making API requests and fetching data\n",
    "    # ...\n",
    "\n",
    "    # Return the fetched data\n",
    "    return fetched_data\n",
    "\n",
    "# Streaming platform ingestion\n",
    "def ingest_from_streaming_platform():\n",
    "    # Code for subscribing to the streaming platform and receiving data\n",
    "    # ...\n",
    "\n",
    "    # Return the received data\n",
    "    return received_data\n",
    "\n",
    "# Data ingestion pipeline\n",
    "def data_ingestion_pipeline():\n",
    "    # Ingest data from database\n",
    "    database_data = ingest_from_database()\n",
    "\n",
    "    # Ingest data from API\n",
    "    api_data = ingest_from_api()\n",
    "\n",
    "    # Ingest data from streaming platform\n",
    "    streaming_data = ingest_from_streaming_platform()\n",
    "\n",
    "    # Store the collected data in a suitable storage system\n",
    "    store_data(database_data, api_data, streaming_data)\n",
    "\n",
    "# Store data\n",
    "def store_data(*datasets):\n",
    "    # Code for storing data in a storage system\n",
    "    # ...\n",
    "\n",
    "    # Example: Store data in a CSV file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"data_{timestamp}.csv\"\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        dataset.to_csv(filename, index=False, mode='a' if i > 0 else 'w')\n",
    "\n",
    "# Run the data ingestion pipeline\n",
    "data_ingestion_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06584a4e-8720-4443-ae4f-fb1e5fef5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# Example code for real-time data ingestion pipeline\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "# Define MQTT callback functions\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    print(\"Connected with result code \" + str(rc))\n",
    "    # Subscribe to the desired topic(s)\n",
    "    client.subscribe(\"sensors/#\")\n",
    "\n",
    "def on_message(client, userdata, msg):\n",
    "    # Process the received sensor data\n",
    "    process_sensor_data(msg.payload)\n",
    "\n",
    "# Process sensor data\n",
    "def process_sensor_data(data):\n",
    "    # Code for processing the sensor data\n",
    "    # ...\n",
    "\n",
    "    # Example: Print the received data\n",
    "    print(data)\n",
    "\n",
    "# Configure and connect to the MQTT broker\n",
    "client = mqtt.Client()\n",
    "client.on_connect = on_connect\n",
    "client.on_message = on_message\n",
    "\n",
    "client.connect(\"mqtt.broker.com\", 1883, 60)\n",
    "\n",
    "# Loop to continuously listen for incoming messages\n",
    "client.loop_forever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572ad19-9529-42d7-846b-9a018e73dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# Example code for data ingestion pipeline handling different file formats\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# CSV ingestion\n",
    "def ingest_from_csv(file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Perform data validation and cleansing\n",
    "    # ...\n",
    "\n",
    "    # Return the processed data\n",
    "    return data\n",
    "\n",
    "# JSON ingestion\n",
    "def ingest_from_json(file_path):\n",
    "    # Read the JSON file into a DataFrame\n",
    "    data = pd.read_json(file_path)\n",
    "\n",
    "    # Perform data validation and cleansing\n",
    "    # ...\n",
    "\n",
    "    # Return the processed data\n",
    "    return data\n",
    "\n",
    "# Data ingestion pipeline for handling different file formats\n",
    "def data_ingestion_pipeline(file_path, file_format):\n",
    "    if file_format == 'csv':\n",
    "        data = ingest_from_csv(file_path)\n",
    "    elif file_format == 'json':\n",
    "        data = ingest_from_json(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format.\")\n",
    "\n",
    "    # Store the processed data or perform further operations\n",
    "    # ...\n",
    "\n",
    "# Run the data ingestion pipeline\n",
    "data_ingestion_pipeline('data.csv', 'csv')\n",
    "data_ingestion_pipeline('data.json', 'json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dec158-9137-40df-8ea3-8d8f66c3e7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7ca294d-96c7-417c-bdb5-e65890e64161",
   "metadata": {},
   "source": [
    "## Model Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e850772-d5f4-433b-aed6-75f076425705",
   "metadata": {},
   "source": [
    "4. Build a machine learning model to predict customer churn based on a given dataset. Train the model using appropriate algorithms and evaluate its performance.\n",
    "5. Develop a model training pipeline that incorporates feature engineering techniques such as one-hot encoding, feature scaling, and dimensionality reduction.\n",
    "6. Train a deep learning model for image classification using transfer learning and fine-tuning techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09b4b1-c8e0-410c-b81e-c1f1f353df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "# Example code for building a customer churn prediction model\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv('churn_dataset.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = dataset.drop('churn', axis=1)\n",
    "y = dataset['churn']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53f803-f0eb-4360-ba22-4af3181913d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# Example code for a model training pipeline with feature engineering\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = dataset.drop('target', axis=1)\n",
    "y = dataset['target']\n",
    "\n",
    "# Perform one-hot encoding on categorical features\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Perform feature scaling on numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# Perform dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c1fbc-0eec-4e4e-9027-aad8fa4bc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "# Example code for training a deep learning model with transfer learning\n",
    "\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom dense layers for the classification task\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=num_epochs, validation_data=validation_generator)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('trained_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121ee29-b148-493d-9875-5d4b96a69ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab402c08-4316-4a2e-af94-b1d351d576ed",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741acdf1-8620-412e-8a11-35c2ff8a9afb",
   "metadata": {},
   "source": [
    "7. Implement cross-validation to evaluate the performance of a regression model for predicting housing prices.\n",
    "8. Perform model validation using different evaluation metrics such as accuracy, precision, recall, and F1 score for a binary classification problem.\n",
    "9. Design a model validation strategy that incorporates stratified sampling to handle imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301cdf5-6814-4b52-8465-fb8e6dcd1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "# Example code for cross-validation of a regression model\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Initialize the regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse_scores = -scores\n",
    "mean_mse = np.mean(mse_scores)\n",
    "\n",
    "print(\"Mean Squared Error:\", mean_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791d4b4-518a-4d1f-9861-da9e89ad137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "# Example code for model validation using evaluation metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21a921-4f84-489b-b7da-c2267aff3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "# Example code for model validation with stratified sampling for imbalanced datasets\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Initialize the support vector classifier\n",
    "model = SVC()\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "accuracies = []\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate the mean accuracy\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e334780b-4ee9-402f-a239-34bd41e1a816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecfb6ef2-d8dc-4a9a-87d4-1d600e07ae8c",
   "metadata": {},
   "source": [
    "##  Deployment Strategy:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b228fdb-0b29-4897-8ddc-7f9467a47ae5",
   "metadata": {},
   "source": [
    "10. Create a deployment strategy for a machine learning model that provides real-time recommendations based on user interactions.\n",
    "11. Develop a deployment pipeline that automates the process of deploying machine learning models to cloud platforms such as AWS or Azure.\n",
    "12. Design a monitoring and maintenance strategy for deployed models to ensure their performance and reliability over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f85dd-483b-470d-8423-55b274661a03",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "10. Creating a deployment strategy for a machine learning model that provides real-time recommendations based on user interactions would depend on the specific requirements and infrastructure in place. It may involve designing APIs, microservices, or event-driven architectures. Providing a specific code example without more context would be challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8156eac-d81a-460b-97da-79e52a8c678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "# Example code for deploying a machine learning model using AWS SageMaker\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Initialize the random forest classifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save the model\n",
    "model_path = \"/path/to/saved_model\"\n",
    "model.save(model_path)\n",
    "\n",
    "# Set up SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "# Create a SageMaker model\n",
    "sagemaker_model = sagemaker_session.create_model(model_path=model_path, role=role)\n",
    "\n",
    "# Deploy the model to an endpoint\n",
    "endpoint_name = \"your-endpoint-name\"\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\", endpoint_name=endpoint_name)\n",
    "\n",
    "# Make predictions using the deployed model\n",
    "input_data = [[5.1, 3.5, 1.4, 0.2], [6.2, 3.4, 5.4, 2.3]]\n",
    "predictor.content_type = \"text/csv\"\n",
    "predictor.serializer = csv_serializer\n",
    "predictions = predictor.predict(input_data)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "# Delete the endpoint\n",
    "predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb3b2d-3482-43a1-9483-d765524c5003",
   "metadata": {},
   "source": [
    "12. Designing a monitoring and maintenance strategy for deployed models to ensure their performance and reliability over time would involve setting up monitoring tools, logging, alerting mechanisms, and periodic model retraining. Providing a specific code example without more context would be challenging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e572e6-6bfd-40fa-b20a-231735eea27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
